---
title: "Homework 6 - P8105 Data Science I"
author: "Yongyan Liu (yl6107)"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include=FALSE}
# Load required packages
library(tidyverse)
library(broom)
library(modelr)

# Set default figure dimensions for all plots
knitr::opts_chunk$set(
  fig.width = 8,
  fig.height = 6
)

# Set default ggplot theme
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

## Problem 1

### Data Loading and Cleaning

Load the Washington Post homicide data, filter to valid numeric ages and White/Black victims, exclude certain cities with data issues, and create variables for city-state identifier and case resolution status.

```{r load_homicide_data}
homicide_df =
  # Load data from GitHub
  read_csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv", show_col_types = FALSE) |>
  # Keep only rows where victim_age contains only digits (filters out "Unknown", etc.)
  filter(str_detect(victim_age, "^[0-9]+$")) |>
  mutate(
    # Create city_state identifier (e.g., "Baltimore, MD")
    city_state = str_c(city, ", ", state),
    # Create binary outcome: 1 = solved (closed by arrest), 0 = unsolved
    solved = ifelse(disposition == "Closed by arrest", 1, 0),
    # Convert victim_age to numeric (safe now since non-numeric values filtered out)
    victim_age = as.numeric(victim_age)
  ) |>
  filter(
    # Exclude cities with data quality issues or missing data
    !city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"),
    # Restrict analysis to White and Black victims as specified
    victim_race %in% c("White", "Black")
  )
```

### Baltimore, MD Analysis

Fit a logistic regression model for Baltimore with resolved vs. unresolved as the outcome and victim age, sex, and race as predictors.

```{r baltimore_glm}
# Filter data to Baltimore only
baltimore_df =
  homicide_df |>
  filter(city_state == "Baltimore, MD")

# Fit logistic regression model
# - Outcome: solved (binary: 1 = solved, 0 = unsolved)
# - Predictors: victim age, sex, and race
baltimore_glm =
  glm(solved ~ victim_age + victim_sex + victim_race,
      data = baltimore_df,
      family = binomial())  # binomial family for logistic regression

# Extract and display the adjusted OR for male vs female victims
baltimore_glm |>
  tidy(conf.int = TRUE, exponentiate = TRUE) |>  # exponentiate = TRUE converts log-odds to odds ratios
  filter(term == "victim_sexMale") |>             # Extract only the sex coefficient
  select(term, estimate, conf.low, conf.high) |>  # Select relevant columns
  knitr::kable(digits = 3)
```

The adjusted odds ratio for solving homicides comparing male victims to female victims is 0.426 (95% CI: 0.325, 0.558), keeping all other variables fixed. This means male victims have approximately 57% lower odds of their case being solved compared to female victims.

### All Cities Analysis

Run logistic regression for each city and extract the adjusted OR and CI for male vs. female victims.

```{r all_cities_glm, fig.width = 10, fig.height = 8}
# Fit logistic regression for each city using nest/map/unnest workflow
city_glm_results =
  homicide_df |>
  nest(data = -city_state) |>  # Create a nested dataframe with one row per city
  mutate(
    # Fit glm to each city's data using map()
    model = map(data, \(df) glm(solved ~ victim_age + victim_sex + victim_race,
                                 data = df, family = binomial())),
    # Extract tidy results with ORs and CIs for each model
    results = map(model, \(x) tidy(x, conf.int = TRUE, exponentiate = TRUE))
  ) |>
  select(city_state, results) |>
  unnest(results) |>                              # Expand the results list-column

  filter(term == "victim_sexMale") |>             # Keep only the sex coefficient
  select(city_state, estimate, conf.low, conf.high)

# Create forest plot of ORs by city
city_glm_results |>
  mutate(city_state = fct_reorder(city_state, estimate)) |>  # Order cities by OR
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "red") +

  coord_flip() +
  labs(
    title = "Adjusted OR for Solving Homicides: Male vs. Female Victims",
    x = "City",
    y = "Adjusted Odds Ratio (95% CI)",
    caption = "Reference line at OR = 1 (no difference)"
  )
```

**Interpretation:** The plot shows the adjusted odds ratios comparing the resolution of homicides for male victims versus female victims across cities. Most cities have ORs below 1, indicating that homicides with male victims are less likely to be solved than those with female victims, after adjusting for victim age and race. New York, NY appears to have one of the lowest ORs, while some cities like Albuquerque, NM have ORs that include 1 in their confidence intervals, suggesting no significant difference by victim sex in those locations. The wide confidence intervals in some cities reflect smaller sample sizes or greater variability.

**Note:** Three cities produced warnings about fitted probabilities being numerically 0 or 1 during confidence interval calculation (profile likelihood). This occurs when the algorithm tests extreme parameter values while profiling the likelihood to find CI bounds. The affected cities are:

- Albuquerque, NM (OR: 1.77, 95% CI: 0.83–3.76)
- Pittsburgh, PA (OR: 0.43, 95% CI: 0.26–0.70)
- Sacramento, CA (OR: 0.67, 95% CI: 0.33–1.31)

These warnings do not affect the validity of the results. All estimates and confidence intervals are finite, properly bounded, and interpretable. The warnings simply indicate numerical boundary conditions encountered during the iterative CI calculation process, not problems with the final estimates.

## Problem 2

### Data Loading

Load the Central Park weather data from the `p8105.datasets` package and fit a simple linear regression with `tmax` as the response and `tmin` and `prcp` as predictors.

```{r weather_data}
# Load the p8105.datasets package for weather data
library(p8105.datasets)

data("ny_noaa")

weather_df =
  ny_noaa |>
  # Central Park weather station
  filter(id == "USW00094728") |>
  # Remove rows with missing temperature or precipitation
  drop_na(tmax, tmin, prcp) |>
  mutate(
    # Convert temperatures from tenths of degrees C to degrees C
    tmax = as.numeric(tmax) / 10,
    tmin = as.numeric(tmin) / 10,
    # Convert precipitation from tenths of mm to mm
    prcp = prcp / 10
  )
```

### Bootstrap Analysis

Perform 5000 bootstrap samples and extract r² (using `glance()`) and β̂₁/β̂₂ (using `tidy()`) from each fitted model.

```{r bootstrap}
# Set seed for reproducibility
set.seed(1)

n_bootstrap = 5000

# Generate bootstrap samples and fit models
bootstrap_results =
  tibble(strap_number = 1:n_bootstrap) |>
  mutate(
    # Create bootstrap sample: same size as original, sampled with replacement
    strap_sample = map(strap_number, \(i) slice_sample(weather_df, n = nrow(weather_df), replace = TRUE)),
    # Fit linear regression: tmax ~ tmin + prcp
    model = map(strap_sample, \(df) lm(tmax ~ tmin + prcp, data = df)),
    # Extract r-squared using glance()
    r_squared = map_dbl(model, \(m) glance(m)$r.squared),
    # Extract coefficient estimates using tidy()
    tidy_results = map(model, tidy)
  ) |>
  select(strap_number, r_squared, tidy_results)

# Extract beta1/beta2 ratio (coefficient for tmin / coefficient for prcp)
beta_ratio =
  bootstrap_results |>
  unnest(tidy_results) |>
  filter(term %in% c("tmin", "prcp")) |>
  select(strap_number, term, estimate) |>
  pivot_wider(names_from = term, values_from = estimate) |>
  mutate(beta_ratio = tmin / prcp)

# Combine r-squared and beta ratio into one dataframe
bootstrap_estimates =
  bootstrap_results |>
  select(strap_number, r_squared) |>
  left_join(beta_ratio |> select(strap_number, beta_ratio), by = "strap_number")
```

### Distribution of Estimates

```{r bootstrap_plots}
# Plot distribution of r-squared estimates
bootstrap_estimates |>
  ggplot(aes(x = r_squared)) +
  geom_histogram(bins = 50, fill = "steelblue", alpha = 0.7) +
  labs(
    title = "Distribution of R-squared from Bootstrap Samples",
    x = "R-squared",
    y = "Count"
  )

# Plot distribution of beta ratio estimates
# Using expression() for proper mathematical notation
bootstrap_estimates |>
  ggplot(aes(x = beta_ratio)) +
  geom_histogram(bins = 50, fill = "steelblue", alpha = 0.7) +
  labs(
    title = expression("Distribution of " * hat(beta)[1] / hat(beta)[2] * " from Bootstrap Samples"),
    x = expression(hat(beta)[tmin] / hat(beta)[prcp]),
    y = "Count"
  )
```

### 95% Confidence Intervals

```{r bootstrap_ci}
# Calculate 95% CI for r-squared using the 2.5th and 97.5th percentiles
r_squared_ci =
  bootstrap_estimates |>
  summarize(
    ci_lower = quantile(r_squared, 0.025),
    ci_upper = quantile(r_squared, 0.975)
  )

r_squared_ci |>
  knitr::kable(digits = 4, caption = "95% CI for R-squared")

# Calculate 95% CI for beta ratio using the 2.5th and 97.5th percentiles
beta_ratio_ci =
  bootstrap_estimates |>
  summarize(
    ci_lower = quantile(beta_ratio, 0.025),
    ci_upper = quantile(beta_ratio, 0.975)
  )

beta_ratio_ci |>
  knitr::kable(digits = 4, caption = "95% CI for β̂₁/β̂₂")
```

**Interpretation:** The r² distribution is approximately symmetric and narrowly concentrated around 0.91, indicating that `tmin` and `prcp` consistently explain about 91% of the variance in `tmax` across bootstrap samples. The narrow 95% CI (0.907, 0.914) reflects the strong and stable linear relationship between these variables.

The β̂₁/β̂₂ distribution shows the ratio of the `tmin` coefficient to the `prcp` coefficient. Since `prcp` consistently has a negative effect on `tmax` while `tmin` has a positive effect, the ratio is consistently negative. The distribution appears roughly symmetric around its mean.

## Problem 3

### Data Loading and Cleaning

Load the birthweight dataset and clean the data by converting appropriate numeric variables to factors and checking for missing data.

```{r load_birthweight}
# Load birthweight data and convert categorical variables to factors
birthweight_df =
  read_csv("data/birthweight.csv", show_col_types = FALSE) |>
  mutate(
    # Convert baby sex: 1 = male, 2 = female
    babysex = factor(babysex, levels = c(1, 2), labels = c("male", "female")),
    # Convert father's race with descriptive labels
    frace = factor(frace, levels = c(1, 2, 3, 4, 8, 9),
                   labels = c("White", "Black", "Asian", "Puerto Rican", "Other", "Unknown")),
    # Convert mother's race with descriptive labels
    mrace = factor(mrace, levels = c(1, 2, 3, 4, 8),
                   labels = c("White", "Black", "Asian", "Puerto Rican", "Other")),
    # Convert malformation indicator: 0 = absent, 1 = present
    malform = factor(malform, levels = c(0, 1), labels = c("absent", "present"))
  )

# Check for missing data across all variables
missing_summary =
  birthweight_df |>
  # Count NAs in each column
  summarize(across(everything(), \(x) sum(is.na(x)))) |>
  # Reshape to long format for easier viewing
  pivot_longer(everything(), names_to = "variable", values_to = "n_missing") |>
  # Keep only variables with missing values
  filter(n_missing > 0)

# Report missing data status
if (nrow(missing_summary) == 0) {
  cat("No missing data in the dataset.\n")
} else {
  print(missing_summary)
}
```

### Proposed Model

Based on literature and biological plausibility, I propose a model that includes factors known to influence birthweight:

- **Baby characteristics:** `blength` (baby length), `gaweeks` (gestational age)
- **Mother characteristics:** `mrace` (mother's race), `smoken` (smoking during pregnancy), `wtgain` (weight gain during pregnancy)

These predictors were chosen because:

- Baby length and gestational age are direct measures of fetal development
- Maternal race is associated with birthweight disparities
- Smoking during pregnancy is a known risk factor for low birthweight
- Maternal weight gain reflects nutritional status during pregnancy

```{r proposed_model}
# Fit linear regression model with predictors selected based on literature
# Predictors: baby length, gestational age, mother's race, smoking, weight gain
proposed_model = lm(bwt ~ blength + gaweeks + mrace + smoken + wtgain, data = birthweight_df)

# Display coefficient estimates with standard errors and p-values
proposed_model |>
  tidy() |>
  knitr::kable(digits = 3)

# Display model fit statistics (R-squared, AIC, BIC)
proposed_model |>
  glance() |>
  select(r.squared, adj.r.squared, AIC, BIC) |>
  knitr::kable(digits = 3)
```

### Residual Plot

Plot residuals against fitted values to assess model assumptions.

```{r residual_plot}
# Create residual vs fitted plot to check model assumptions
# - Random scatter around 0 indicates linearity assumption is met
# - Constant spread indicates homoscedasticity
birthweight_df |>
  add_predictions(proposed_model) |>
  add_residuals(proposed_model) |>
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = 0.3) +
  # Reference line at 0
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(
    title = "Residuals vs Fitted Values for Proposed Model",
    x = "Fitted Values (predicted birthweight in grams)",
    y = "Residuals"
  )
```

### Model Comparison

Compare the proposed model with two alternatives using cross-validated prediction error. We use RMSE (Root Mean Squared Error) as the primary metric for comparison because:

- RMSE is in the same units as the outcome (grams), making it directly interpretable
- It penalizes larger prediction errors more heavily than smaller ones, which is appropriate when large errors are particularly undesirable
- Cross-validated RMSE estimates out-of-sample prediction accuracy, avoiding overfitting bias that can occur with in-sample metrics like R²

The three models compared are:

- **Model 1 (Proposed):** `bwt ~ blength + gaweeks + mrace + smoken + wtgain`
- **Model 2:** `bwt ~ blength + gaweeks` (main effects only)
- **Model 3:** `bwt ~ bhead * blength * babysex` (all interactions including three-way)

```{r model_comparison}
# Define the three models as functions that take a dataframe and return a fitted model
# Model 1 (Proposed): includes predictors based on literature review
model_1 = \(df) lm(bwt ~ blength + gaweeks + mrace + smoken + wtgain, data = df)
# Model 2 (Simple): main effects only - length and gestational age
model_2 = \(df) lm(bwt ~ blength + gaweeks, data = df)
# Model 3 (Interactions): head circumference, length, and sex with all interactions
model_3 = \(df) lm(bwt ~ bhead * blength * babysex, data = df)

# Set seed for reproducibility of cross-validation splits
set.seed(1)

# Perform Monte Carlo cross-validation with 100 random 80/20 train/test splits
cv_results =
  crossv_mc(birthweight_df, n = 100) |>  # 100 random splits, default 80% train / 20% test
  mutate(
    # Convert resample objects to tibbles for model fitting
    train = map(train, as_tibble),
    test = map(test, as_tibble),
    # Fit each model to each training set
    model_proposed = map(train, model_1),
    model_length_ga = map(train, model_2),
    model_interactions = map(train, model_3),
    # Calculate RMSE on test set for each model
    # map2_dbl applies function to pairs of (model, test_data) and returns numeric
    rmse_proposed = map2_dbl(model_proposed, test, \(mod, df) rmse(mod, df)),
    rmse_length_ga = map2_dbl(model_length_ga, test, \(mod, df) rmse(mod, df)),
    rmse_interactions = map2_dbl(model_interactions, test, \(mod, df) rmse(mod, df))
  )

# Summarize RMSE across all cross-validation folds
cv_results |>
  select(starts_with("rmse")) |>
  # Reshape to long format for summary
  pivot_longer(everything(), names_to = "model", values_to = "rmse", names_prefix = "rmse_") |>
  group_by(model) |>
  # Calculate mean and SD of RMSE across 100 CV folds
  summarize(
    mean_rmse = mean(rmse),
    sd_rmse = sd(rmse)
  ) |>
  arrange(mean_rmse) |>  # Sort by best (lowest) RMSE
  knitr::kable(digits = 2)
```

### RMSE Distribution Plot

```{r rmse_plot}
# Visualize RMSE distribution across CV folds using violin plots
# Violin plots show the full distribution, not just summary statistics
cv_results |>
  select(starts_with("rmse")) |>
  # Reshape to long format for ggplot
  pivot_longer(everything(), names_to = "model", values_to = "rmse", names_prefix = "rmse_") |>
  # Reorder models by median RMSE (best to worst)
  mutate(model = fct_reorder(model, rmse)) |>
  ggplot(aes(x = model, y = rmse, fill = model)) +
  geom_violin() +  # Violin plot shows distribution shape
  labs(
    title = "Cross-Validated RMSE by Model",
    x = "Model",
    y = "RMSE"
  ) +
  theme(legend.position = "none")  # Legend redundant with x-axis labels
```

**Interpretation:** The cross-validated RMSE comparison shows that the interactions model (`bhead * blength * babysex`) has the lowest prediction error (mean RMSE ≈ 289), followed by the proposed model (mean RMSE ≈ 318), and the simple main effects model (mean RMSE ≈ 332).

The interactions model performs best because head circumference (`bhead`) is a strong predictor of birthweight, and allowing for interactions between head circumference, body length, and sex captures important relationships. The proposed model, while including relevant predictors like maternal race and smoking, does not include head circumference, which explains its higher RMSE. The simple model with only length and gestational age has the highest error due to omitting important predictors.

From the residual plot of the proposed model, the residuals are mostly centered around zero, though there appears to be some heteroscedasticity with larger spread at higher fitted values and a few outliers with very low birthweights. This suggests the model may not fully capture variation at the extremes of the birthweight distribution.
